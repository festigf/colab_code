# -*- coding: utf-8 -*-
"""gf-email-sms-spam-classification-nltk-nb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11XVwTI8T3gnz79I01zC4XYSC9Co29Q1C
"""

!pip install wordcloud -q
import pandas as pd
import numpy as np

#df = pd.read_csv("/kaggle/input/sms-spam-collection-dataset/spam.csv", encoding='latin1')
df = pd.read_csv('https://raw.githubusercontent.com/festigf/colabs/main/bayes_spam_classifier/SMSSpamCollection_tab', sep='\t',header=None, names=['v1', 'v2'])
#df = pd.read_csv('/kaggle/input/smsspamcollection/SMSSpamCollection', encoding='latin1', sep='\t',header=None, names=['v1', 'v2'])

df.head()

df.shape

"""# Data Cleaning"""

df.info()

#df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)

df.head()

df.rename(columns={'v1':'target','v2':'text'}, inplace=True)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

df.isnull().sum()

df.duplicated().sum()

"""pandas.DataFrame.drop_duplicates
DataFrame.drop_duplicates(subset=None, *, keep='first', inplace=False, ignore_index=False)[source]
Return DataFrame with duplicate rows removed.

Considering certain columns is optional. Indexes, including time indexes are ignored.

Parameters:
subsetcolumn label or sequence of labels, optional
Only consider certain columns for identifying duplicates, by default use all of the columns.

keep{‘first’, ‘last’, False}, default ‘first’
Determines which duplicates (if any) to keep.

‘first’ : Drop duplicates except for the first occurrence.
"""

df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

"""# EDA"""

df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(),labels=['ham', 'spam'], autopct="%0.2f")
plt.show()

"""https://www.askpython.com/python-modules/nltk-punkt#:~:text=In%20NLTK%2C%20PUNKT%20is%20an,referred%20to%20as%20unlabeled%20data.)
https://www.nltk.org/install.html
"""

#!pip install --user -U nltk

import nltk
nltk.download('punkt',quiet=True)

df['characters'] = df['text'].apply(len)

df['words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

df['sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['characters','words','sentences']].describe()

#for ham messages
df[df['target'] == 0][['characters','words','sentences']].describe()

#for spam messages
df[df['target'] == 1][['characters','words','sentences']].describe()

import seaborn as sns

sns.histplot(df[df['target'] == 0]['characters']);
sns.histplot(df[df['target'] == 1]['characters'],color='orange');

sns.histplot(df[df['target'] == 0]['words']);
sns.histplot(df[df['target'] == 1]['words'],color='orange');

sns.pairplot(df,hue='target');

numeric_df = df.select_dtypes(include=['float64','int32','int64'])
# Calculate the correlation matrix
correlation_matrix = numeric_df.corr()
sns.heatmap(data=correlation_matrix, annot=True)
plt.show()

"""# Data Preprocessing"""

import string
string.punctuation
import nltk
nltk.download('stopwords')
#from nltk.corpus import stopwords
#english_stopwords = stopwords.words('english')
english_stopwords = nltk.corpus.stopwords.words('english')

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def transform(text):
    text = text.lower()                     #Lower_case
    text = nltk.word_tokenize(text)         #word_tokenization

    y=[]
    for i in text:
        if i.isalnum():                     #removing non alpha-numeric characters
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:                          #removing stopwords and punctuations
        if i not in english_stopwords and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))                #stemming

    return " ".join(y)

df['transformed_text'] = df['text'].apply(transform)



df.head()

from wordcloud import WordCloud
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc = wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=" "))     # for spam

plt.figure(figsize=(13,6))        #for spam
plt.imshow(spam_wc);

ham_wc = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=" "))     #for ham

plt.figure(figsize=(13,6))        #for ham
plt.imshow(ham_wc);

spam_corpus = []
for message in df[df['target'] == 1]['transformed_text'].tolist():
    for word in message.split():
        spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
word_counter = Counter(spam_corpus)

# Convert Counter data to a DataFrame
word_counts_df = pd.DataFrame(word_counter.most_common(30), columns=['Word', 'Count'])

# Create a barplot using seaborn
sns.barplot(x='Word', y='Count', data=word_counts_df)

plt.xticks(rotation=45)
plt.show()

ham_corpus = []
for message in df[df['target'] == 0]['transformed_text'].tolist():
    for word in message.split():
        ham_corpus.append(word)

len(ham_corpus)

word_counter2 = Counter(ham_corpus)

# Convert Counter data to a DataFrame
word_counts_df2 = pd.DataFrame(word_counter2.most_common(30), columns=['Word', 'Count'])

# Create a barplot using seaborn
sns.barplot(x='Word', y='Count', data=word_counts_df2)

plt.xticks(rotation=45)
plt.show()

"""# Model Building"""

# from sklearn.feature_extraction.text import TfidfVectorizer
# tfidf = TfidfVectorizer(max_features=3000)

# X = tfidf.fit_transform(df['transformed_text']).toarray()

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=3000)
X = vectorizer.fit_transform(df['transformed_text'].values).toarray()

X.shape

vectorizer.get_feature_names_out()

y = df['target'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=2,stratify=y)

from sklearn.naive_bayes import MultinomialNB                  #performs best with best precision
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

mnb = MultinomialNB()

#Here MultinomialNB is best, as data is imbalanced, so precision matters alot.
mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
# Print accuracy score
print("Accuracy Score:", accuracy_score(y_test, y_pred2))

# Print confusion matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred2))

# Print precision score
print("Precision Score:", precision_score(y_test, y_pred2))

import pickle
#pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(vectorizer,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))

"""## implemetazione classe  Naive Bayes come da corso UNITN
### class NaiveBayesStandard
"""

import numpy as np,pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import datasets
import matplotlib.pyplot as plt
from IPython.display import display, Math
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
np.seterr(divide='ignore', invalid='ignore')

# Naive Bayes come da corso UNITN
class NaiveBayesStandard:

    def __init__(self,soglia=0.26,alpha=1):
      self.soglia=soglia
      self.alpha=alpha

    def fit(self, X, y):
        self.n_samples, self.n_features = X.shape

        self._classes = np.unique(y)
        n_classes = len(self._classes)

        self.P_spam_parole1=np.array([(np.sum(X[y==1,c])/np.sum(y)).ravel()[0] for c in range(X.shape[1])])
        self.P_spam_parole0 = 1-self.P_spam_parole1

        self.P_ham_parole1=np.array([(np.sum(X[y==0,c])/(len(y)-np.sum(y))).ravel()[0] for c in range(X.shape[1])])
        self.P_ham_parole0 = 1-self.P_ham_parole1

        self.P_spam=np.sum(y)/len(y)
        self.P_ham=1-self.P_spam

    def predict(self, X):
        self.spamworld=np.array([np.prod((self.P_spam_parole1*R) +((1-R)*self.P_spam_parole0)) for R in X])
        self.hamworld=np.array([np.prod((self.P_ham_parole1*R) +((1-R)*self.P_ham_parole0)) for R in X])
        posterior = (self.spamworld*self.P_spam)/(self.spamworld*self.P_spam+self.hamworld*self.P_ham)

        # implementazione metodo 2 UNITN: funziona solo con poche parole!! (lentissimo!)
        # sw=np.ones(X.shape[0])
        # hw=np.ones(X.shape[0])

        # for j in range(X.shape[0]):
        #   for i in range(X.shape[1]):
        #     sw[j] = sw[j] * self.P_spam_parole1[i] if (X[j,i]>=1) else self.P_spam_parole0[i]
        #     hw[j] = hw[j] * self.P_ham_parole1[i] if (X[j,i]>=1) else self.P_ham_parole0[i]
        # posterior = sw*self.P_spam/(sw*self.P_spam+hw*self.P_ham)

        classe = pd.Series(np.where(posterior>self.soglia,1,0),name="Posterior")
        return classe

# Naive Bayes come da corso UNITN variante calcolo con sommatoria di log
class NaiveBayesLog:

    def __init__(self,soglia=0.26):
      self.soglia=soglia

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self._classes = np.unique(y)
        n_classes = len(self._classes)

        self.P_spam_parole1=np.array([(np.sum(X[y==1,c])/np.sum(y)).ravel()[0] for c in range(X.shape[1])])
        self.P_spam_parole0 = 1-self.P_spam_parole1

        self.P_ham_parole1=np.array([(np.sum(X[y==0,c])/(len(y)-np.sum(y))).ravel()[0] for c in range(X.shape[1])])
        self.P_ham_parole0 = 1-self.P_ham_parole1

        self.P_spam=np.sum(y)/len(y)
        self.P_ham=1-self.P_spam

    def predict(self, X):
        #correzione alpha Laplace smoothing (x+alpha)/(y+alpha) --> alpha =1 quando y=0??
        sw=np.array([np.sum(np.log( (self.P_spam_parole1*R) +((1-R)*self.P_spam_parole0) )) for R in X])
        hw=np.array([np.sum(np.log((self.P_ham_parole1*R) +((1-R)*self.P_ham_parole0))) for R in X])

        alpha=np.array(np.maximum(sw.ravel(),hw.ravel()))
        self.spamworld=np.exp(sw+alpha)
        self.hamworld=np.exp(hw+alpha)
        posterior = (self.spamworld*self.P_spam)/(self.spamworld*self.P_spam+self.hamworld*self.P_ham)
        classe = pd.Series(np.where(posterior>self.soglia,1,0),name="Posterior")
        return classe

nb = NaiveBayesStandard(soglia=0.5)
#nb = NaiveBayesLog(soglia=0.7)
nb.fit(X_train, y_train)

posterior=nb.predict(X_test)

def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

print("accuratezza: {:.2%}".format(accuracy(y_test,posterior)))

tab = pd.crosstab(index = y_test,columns=posterior,margins=False)
ax = plt.axes()
ax.set_title('Confusion matrix')
sns.heatmap(tab, annot=True,cmap='Blues', fmt='.3g', ax = ax);

"""## ricerca soglia"""

# ricerca soglia
soglie=np.linspace(0,1,11)
accuratezze=np.array([])
TPTN=()
maxAccuratezza=0
maxSoglia=0

for s in soglie:
  nb = NaiveBayesStandard(soglia=s)
  nb.fit(X_train, y_train)
  posterior=nb.predict(X_test)

  # post=np.where(posterior>s,1,0)
  # controllo=np.where(post==dataTest["Predictions"],1,0)
  tab = pd.crosstab(index = y_test,columns=posterior.values,margins=False)
  ax = plt.axes()
  ax.set_title('Confusion matrix - soglia: {:.2%}'.format(s))
  sns.heatmap(tab, annot=True,cmap='Blues', fmt='.3g', ax = ax);
  plt.show()
  accuratezza=np.diag(tab).sum() / tab.to_numpy().sum()
  if accuratezza>maxAccuratezza:
    maxAccuratezza=accuratezza
    maxSoglia=s
    TPTN=(np.diag(tab))
  accuratezze=np.append(accuratezze,accuratezza)

#print("soglia:",np.argmax(accuratezze),"accuratezza:",np.max(accuratezze))
print("soglia:",maxSoglia,"accuratezza:",max)
sp=sns.scatterplot(data=pd.DataFrame({"soglie":soglie,"Accuratezza":accuratezze}), x="soglie", y="Accuratezza");
sp.text(maxSoglia,maxAccuratezza,TPTN);